# DataQualityAgent  
**An Agentic Data Quality System for E-commerce Analytics (DuckDB + dbt + FastAPI)**

![CI](https://github.com/manishdev92/dataqualityagent/actions/workflows/ci.yml/badge.svg)

---

## ğŸ“Œ What is this project?

**DataQualityAgent** is a local, end-to-end **data quality automation system** built for data engineers and analytics engineers.

It simulates a real analytics warehouse and automatically:

- profiles tables
- detects data quality issues
- generates **dbt tests**
- produces a **human-readable report**
- exposes everything via a **REST API**
- validates logic using **pytest**
- runs **CI on every push using GitHub Actions**

This project is designed to be:
- âœ… beginner-friendly
- âœ… realistic (not toy data)
- âœ… reusable in real projects
- âœ… impressive for recruiters

---

## ğŸ¯ Problem this project solves

In real data platforms, data breaks due to:

- duplicate primary keys  
- sudden null spikes  
- broken foreign keys  
- unexpected value spikes (currency / ingestion bugs)  
- silent data drift  

Most teams:
- detect issues **late**
- write tests **manually**
- debug issues **reactively**

**This project automates that workflow.**

---

## ğŸ§  What makes this project â€œagenticâ€?

Instead of hardcoding rules, the system follows a **reasoning pipeline**:

1. **Profile** tables (facts + dimensions)
2. **Compare** current data vs baseline
3. **Detect anomalies** based on evidence
4. **Decide which tests are needed**
5. **Generate dbt tests automatically**
6. **Explain findings in a report**

This mirrors how a real data engineer reasons about data quality.

---

## ğŸ— High-level architecture
```
DuckDB (local warehouse)
        â†“
Table Profiler
        â†“
Anomaly Detector (baseline vs today)
        â†“
dbt Test Generator (schema.yml)
        â†“
Markdown DQ Report
        â†“
FastAPI Service
```

---

## ğŸ“¦ Dataset used (realistic)

A realistic **e-commerce star schema**:

### Tables
- `dim_customers`
- `fact_orders`
- `fact_payments`

### Data modes
- **baseline** â†’ clean, healthy data
- **bad_day** â†’ injected issues:
  - duplicate IDs
  - null spikes
  - foreign key violations
  - outlier amounts

This lets you *see the system actually working*.

---

## ğŸ“ Project structure
```
dataqualityagent/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ seed_warehouse.py      # creates baseline / bad_day data
â”‚   â””â”€â”€ run_dq.py              # one-command end-to-end run
â”‚
â”œâ”€â”€ src/dqa/
â”‚   â”œâ”€â”€ connectors/            # DuckDB connector
â”‚   â”œâ”€â”€ profiling/             # table profiling logic
â”‚   â”œâ”€â”€ anomaly/               # anomaly detection rules
â”‚   â”œâ”€â”€ dbtgen/                # dbt schema.yml generator
â”‚   â”œâ”€â”€ reporting/             # markdown report generator
â”‚   â””â”€â”€ api/                   # FastAPI service
â”‚
â”œâ”€â”€ generated/
â”‚   â”œâ”€â”€ dbt/models/schema.yml  # auto-generated dbt tests
â”‚   â””â”€â”€ reports/               # DQ reports
â”‚
â”œâ”€â”€ tests/                     # pytest unit + smoke tests
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```


---

## âš™ï¸ Setup (Mac / Linux)

### 1ï¸âƒ£ Create virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip```

## 2ï¸âƒ£ Install dependencies
- pip install -e ".[dev]"

## â–¶ï¸ Run the system (one command)
- python scripts/run_dq.py --target bad_day

## What this command does

- Seeds baseline data

- Profiles baseline tables

- Seeds bad_day data

- Profiles current tables

- Detects anomalies

- Generates dbt tests

- Writes a DQ report

- ğŸ“„ Outputs to check

## 1ï¸âƒ£ Generated dbt tests
- generated/dbt/models/schema.yml


## Example tests:

- unique

- not_null

- relationships

- accepted_range

## 2ï¸âƒ£ Data Quality report
generated/reports/run_<run_id>.md


## Contains:

- findings grouped by table

- severity (CRITICAL / WARN)

- evidence summary

## ğŸŒ Run as an API (FastAPI)
- Start the service
```uvicorn dqa.api.main:app --reload --port 8000```

## Open Swagger UI
```http://127.0.0.1:8000/docs```

## Run DQ via API
```curl -X POST "http://127.0.0.1:8000/dq/run?target=bad_day"```

## Fetch the report
```curl "http://127.0.0.1:8000/dq/report/<run_id>"```

## ğŸ§ª Tests
```pytest -q```


## Includes:

- unit tests for anomaly logic

- smoke test for full pipeline

- ğŸ”„ Continuous Integration (CI)

- This project uses GitHub Actions for Continuous Integration.

## What runs automatically

- On every push or pull request to main:

- Python 3.11 environment is created

- Project dependencies are installed

- All pytest unit and smoke tests are executed

## Why this matters

- Prevents broken code from being merged

- Ensures data quality logic stays correct

- Demonstrates production-grade engineering practices

- CI status is visible in the Actions tab of the repository.

## ğŸ” How to reuse this in real projects

- You can extend this system to:

- Postgres / Snowflake / BigQuery

- Parquet / S3 / Delta Lake

- dbt CI pipelines

- Airflow / Dagster scheduled runs

- Only the connector layer needs to change.

## ğŸš€ Future enhancements

- Persist historical baselines (trend over time)

- Smarter drift detection (MAD / robust z-score)

- LLM-based test recommendations

- Data Quality score per table

- Docker support

## â­ Why this project stands out

- This is not a tutorial toy.

- It demonstrates:

- system design thinking

- real-world data issues

- automation mindset

- clean Python engineering

- CI/CD awareness

## Ideal for:

- GitHub portfolio

- interviews

- internal tooling inspiration
